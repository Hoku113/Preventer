{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "from IPython.display import clear_output, display\n",
    "from openvino.runtime import Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where model will be downloaded\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# model name as named in Open Model Zoo\n",
    "model_name = \"human-pose-estimation-3d-0001\"\n",
    "# selected precision (FP32, FP16)\n",
    "precision = \"FP32\"\n",
    "\n",
    "BASE_MODEL_NAME = f\"{base_model_dir}/public/{model_name}/{model_name}\"\n",
    "model_path = Path(BASE_MODEL_NAME).with_suffix(\".pth\")\n",
    "onnx_path = Path(BASE_MODEL_NAME).with_suffix(\".onnx\")\n",
    "\n",
    "ir_model_path = f\"model/public/{model_name}/{precision}/{model_name}.xml\"\n",
    "model_weights_path = f\"model/public/{model_name}/{precision}/{model_name}.bin\"\n",
    "\n",
    "if not model_path.exists():\n",
    "    download_command = (\n",
    "        f\"omz_downloader \" f\"--name {model_name} \" f\"--output_dir {base_model_dir}\"\n",
    "    )\n",
    "    ! $download_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Converting human-pose-estimation-3d-0001 to ONNX\n",
      "Conversion to ONNX command: c:\\users\\hokuto\\desktop\\project\\preventer\\.venv\\scripts\\python.exe -- c:\\users\\hokuto\\desktop\\project\\preventer\\.venv\\lib\\site-packages\\openvino\\model_zoo\\internal_scripts\\pytorch_to_onnx.py --model-path=model\\public\\human-pose-estimation-3d-0001 --model-name=PoseEstimationWithMobileNet --model-param=is_convertible_by_mo=True --import-module=model --weights=model\\public\\human-pose-estimation-3d-0001/human-pose-estimation-3d-0001.pth --input-shape=1,3,256,448 --input-names=data --output-names=features,heatmaps,pafs --output-file=model\\public\\human-pose-estimation-3d-0001/human-pose-estimation-3d-0001.onnx\n",
      "\n",
      "ONNX check passed successfully.\n",
      "\n",
      "========== Converting human-pose-estimation-3d-0001 to IR (FP32)\n",
      "Conversion command: c:\\users\\hokuto\\desktop\\project\\preventer\\.venv\\scripts\\python.exe -- C:\\Users\\hokuto\\Desktop\\Project\\Preventer\\.venv\\Scripts\\mo.exe --framework=onnx --data_type=FP32 --output_dir=model\\public\\human-pose-estimation-3d-0001\\FP32 --model_name=human-pose-estimation-3d-0001 --input=data --mean_values=data[128.0,128.0,128.0] --scale_values=data[255.0,255.0,255.0] --output=features,heatmaps,pafs --input_model=model\\public\\human-pose-estimation-3d-0001/human-pose-estimation-3d-0001.onnx --layout=data(NCHW) \"--input_shape=[1, 3, 256, 448]\"\n",
      "\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \tc:\\Users\\hokuto\\Desktop\\Project\\Preventer\\model\\public\\human-pose-estimation-3d-0001/human-pose-estimation-3d-0001.onnx\n",
      "\t- Path for generated IR: \tc:\\Users\\hokuto\\Desktop\\Project\\Preventer\\model\\public\\human-pose-estimation-3d-0001\\FP32\n",
      "\t- IR output name: \thuman-pose-estimation-3d-0001\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tdata\n",
      "\t- Output layers: \tfeatures,heatmaps,pafs\n",
      "\t- Input shapes: \t[1, 3, 256, 448]\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tdata(NCHW)\n",
      "\t- Mean values: \tdata[128.0,128.0,128.0]\n",
      "\t- Scale values: \tdata[255.0,255.0,255.0]\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "OpenVINO runtime found in: \tc:\\users\\hokuto\\desktop\\project\\preventer\\.venv\\lib\\site-packages\\openvino\n",
      "OpenVINO runtime version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "Model Optimizer version: \t2022.1.0-7019-cdb9bec7210-releases/2022/1\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: c:\\Users\\hokuto\\Desktop\\Project\\Preventer\\model\\public\\human-pose-estimation-3d-0001\\FP32\\human-pose-estimation-3d-0001.xml\n",
      "[ SUCCESS ] BIN file: c:\\Users\\hokuto\\Desktop\\Project\\Preventer\\model\\public\\human-pose-estimation-3d-0001\\FP32\\human-pose-estimation-3d-0001.bin\n",
      "[ SUCCESS ] Total execution time: 3.04 seconds. \n",
      "It's been a while, check for a new version of Intel(R) Distribution of OpenVINO(TM) toolkit here https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit/download.html?cid=other&source=prod&campid=ww_2022_bu_IOTG_OpenVINO-2022-1&content=upg_all&medium=organic or on the GitHub*\n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not onnx_path.exists():\n",
    "    convert_command = (\n",
    "        f\"omz_converter \"\n",
    "        f\"--name {model_name} \"\n",
    "        f\"--precisions {precision} \"\n",
    "        f\"--download_dir {base_model_dir} \"\n",
    "        f\"--output_dir {base_model_dir}\"\n",
    "    )\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (tags/v3.9.5:0a7dcbd, May  3 2021, 17:27:52) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66dfb7182981ecd9ebf2f5930af32126a2a6f0db9bde609e6b11b1e02afad2d7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
